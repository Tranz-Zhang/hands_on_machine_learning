{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12\n",
    "Implement Logistic Regression with Mini-batch Gradient Descent using TensorFlow. Train it and evaluate it on the moons dataset (introduced in Chapter 5). Try adding all the bells and whistles:\n",
    "\n",
    "• Define the graph within a logistic_regression() function that can be reused easily.\n",
    "\n",
    "• Save checkpoints using a Saver at regular intervals during training, and save the final model at the end of training.\n",
    "\n",
    "• Restore the last checkpoint upon startup if training was interrupted.\n",
    "\n",
    "• Define the graph using nice scopes so the graph looks good in TensorBoard.\n",
    "\n",
    "• Add summaries to visualize the learning curves in TensorBoard.\n",
    "\n",
    "• Try tweaking some hyperparameters such as the learning rate or the mini- batch size and look at the shape of the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "X_train = iris['data'][:, 2:] # patal width + patel length\n",
    "y_train = (iris['target'] == 2).astype(np.float32) # 1 if Iris-virginica, else 0\n",
    "y_train = y_train.reshape(150, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_plus = np.c_[np.ones((X_train.shape[0], 1)), X_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cost_data(cost_val):\n",
    "    plt.plot(range(len(cost_val)), cost_val, 'b-')\n",
    "    plt.xlabel('n_epochs')\n",
    "    plt.ylabel('cost')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost function\n",
    "def build_logistic_regression_graph(learning_rate):\n",
    "    with tf.name_scope(\"Input\"):\n",
    "        X = tf.placeholder(tf.float32, shape=(None, 3), name=\"X\")\n",
    "        y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "    theta = tf.Variable(tf.random_uniform([3, 1], -1.0, 1.0), name=\"Theta\")\n",
    "    with tf.name_scope(\"Cost_Function\"):\n",
    "        sigmoid = tf.sigmoid(tf.matmul(X, theta), name=\"sigmoid\")\n",
    "        cost_part_1 = tf.multiply(y, tf.log(sigmoid))\n",
    "        cost_part_2 = tf.multiply(tf.subtract(1.0, y), tf.log(tf.subtract(1.0, sigmoid)))\n",
    "        cost = tf.negative(tf.reduce_sum(tf.add(cost_part_1, cost_part_2)))\n",
    "    with tf.name_scope(\"Train\"):\n",
    "        gradient = tf.gradients(cost, [theta])[0]\n",
    "        training_op = tf.assign(theta, theta - learning_rate * gradient, name=\"training_op\")\n",
    "    with tf.name_scope(\"Init\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "    with tf.name_scope(\"Save\"):\n",
    "        saver = tf.train.Saver()\n",
    "    return X, y, training_op, theta, cost, init, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "X, y, training_op, theta, cost, init, saver = build_logistic_regression_graph(0.001)\n",
    "cost_val = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(n_epochs):\n",
    "        sess.run(training_op, feed_dict={X : X_train_plus, y : y_train})\n",
    "        cost_val.append(cost.eval(feed_dict={X : X_train_plus, y : y_train}))\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)\n",
    "plot_cost_data(cost_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 500\n",
    "learning_rate = 0.001\n",
    "\n",
    "X, y, training_op, theta, cost, init, saver = build_logistic_regression_graph(0.001)\n",
    "cost_val = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(n_epochs):\n",
    "        sess.run(training_op, feed_dict={X : X_train_plus, y : y_train})\n",
    "        cost_val.append(cost.eval(feed_dict={X : X_train_plus, y : y_train}))\n",
    "        if i % 100 == 0:\n",
    "            saver.save(sess, '/Users/chance/machine_learning/hands_on_machine_learning/tmp/ex9_12_save.ckpt')\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)\n",
    "plot_cost_data(cost_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue training\n",
    "n_epochs = 1500\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '/Users/chance/machine_learning/hands_on_machine_learning/tmp/ex9_12_save.ckpt')\n",
    "    for i in range(n_epochs):\n",
    "        sess.run(training_op, feed_dict={X : X_train_plus, y : y_train})\n",
    "        cost_val.append(cost.eval(feed_dict={X : X_train_plus, y : y_train}))\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)\n",
    "plot_cost_data(cost_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def create_log_file_path():\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_dir = \"tf_logs\"\n",
    "    logdir = \"{}/run-{}\".format(root_dir, now)\n",
    "    return logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.002\n",
    "\n",
    "X, y, training_op, theta, cost, init, saver = build_logistic_regression_graph(0.001)\n",
    "\n",
    "log_dir = create_log_file_path()\n",
    "cost_summary = tf.summary.scalar(\"Cost\", cost)\n",
    "file_writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(n_epochs):\n",
    "        sess.run(training_op, feed_dict={X : X_train_plus, y : y_train})\n",
    "        summary_str = cost_summary.eval(feed_dict={X : X_train_plus, y : y_train})\n",
    "        file_writer.add_summary(summary_str, i)\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)\n",
    "file_writer.flush()\n",
    "file_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
