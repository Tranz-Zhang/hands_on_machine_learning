{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Classifier Preprocess\n",
    "\n",
    "Write a data preparation pipeline to convert each email into a feature vector. Your preparation pipeline should transform an email into a (sparse) vector indicating the presence or absence of each possible word. For example, if all emails only ever contain four words, “Hello,” “how,” “are,” “you,” then the email “Hello you Hello Hello you” would be converted into a vector [1, 0, 0, 1] (meaning [“Hello” is present, “how” is absent, “are” is absent, “you” is present]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file path:./easy_ham/2170.78c282a5e417d6d231dc75aa8588ebb7\n"
     ]
    }
   ],
   "source": [
    "# list files\n",
    "import os\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "test_dir = \"./easy_ham/\" # \"./hard_ham/\" #\n",
    "files = os.listdir(test_dir)\n",
    "# files = [f for f in os.listdir('./easy_ham/') if os.path.isfile('./easy_ham/' + f)]\n",
    "test_file_path = test_dir + files[0]\n",
    "print(\"Test file path:\" + test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From rssfeeds@jmason.org  Mon Sep 30 13:43:46 2002\n",
      "Return-Path: <rssfeeds@example.com>\n",
      "Delivered-To: yyyy@localhost.example.com\n",
      "Received: from localhost (jalapeno [127.0.0.1])\n",
      "\tby jmason.org (Postfix) with ESMTP id AE79816F16\n",
      "\tfor <jm@localhost>; Mon, 30 Sep 2002 13:43:46 +0100 (IST)\n",
      "Received: from jalapeno [127.0.0.1]\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\tfor jm@localhost (single-drop); Mon, 30 Sep 2002 13:43:46 +0100 (IST)\n",
      "Received: from dogma.slashnull.org (localhost [127.0.0.1]) by\n",
      "    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g8U81fg21359 for\n",
      "    <jm@jmason.org>; Mon, 30 Sep 2002 09:01:41 +0100\n",
      "Message-Id: <200209300801.g8U81fg21359@dogma.slashnull.org>\n",
      "To: yyyy@example.com\n",
      "From: gamasutra <rssfeeds@example.com>\n",
      "Subject: Priceless Rubens works stolen in raid on mansion\n",
      "Date: Mon, 30 Sep 2002 08:01:41 -0000\n",
      "Content-Type: text/plain; encoding=utf-8\n",
      "Lines: 6\n",
      "X-Spam-Status: No, hits=-527.4 required=5.0\n",
      "\ttests=AWL,DATE_IN_PAST_03_06,T_URI_COUNT_0_1\n",
      "\tversion=2.50-cvs\n",
      "X-Spam-Level: \n",
      "\n",
      "URL: http://www.newsisfree.com/click/-1,8381145,215/\n",
      "Date: 2002-09-30T03:04:58+01:00\n",
      "\n",
      "*Arts:* Fourth art raid on philanthropist's home once targeted by the IRA and \n",
      "Dublin gangster Martin Cahill.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read files\n",
    "# test_file_path = \"./spam/00500.85b72f09f6778a085dc8b6821965a76f\"\n",
    "# test_file_path = \"./hard_ham/0239.34e6b6125909c0d998370aacc82daefe\"\n",
    "test_file = open(test_file_path, \"r\", errors='ignore')\n",
    "content = test_file.read()\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "- **Lower-casing:** The entire email is converted into lower case, so that captialization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "\n",
    "- **Stripping HTML:** All HTML tags are removed from the emails. Many emails often come with HTML formatting; we remove all the HTML tags, so that only the content remains.\n",
    "\n",
    "- **Normalizing URLs:** All URLs are replaced with the text “httpaddr”.\n",
    "\n",
    "- **Normalizing Email Addresses:** All email addresses are replaced with the text “emailaddr”.\n",
    "\n",
    "- **Normalizing Numbers:** All numbers are replaced with the text “number”.\n",
    "\n",
    "- **Normalizing Dollars:** All dollar signs ($) are replaced with the text “dollar”.\n",
    "\n",
    "- **Word Stemming:** Words are reduced to their stemmed form. For ex- ample, “discount”, “discounts”, “discounted” and “discounting” are all replaced with “discount”. Sometimes, the Stemmer actually strips off additional characters from the end, so “include”, “includes”, “included”, and “including” are all replaced with “includ”.\n",
    "\n",
    "- **Removal of non-words:** Non-words and punctuation have been re- moved. All white spaces (tabs, newlines, spaces) have all been trimmed to a single space character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "\n",
    "import re\n",
    "import email\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "    \n",
    "def strip_http_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()\n",
    "\n",
    "def replace_url(input_str):\n",
    "    return re.sub(r\"https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\", 'httpaddr', input_str)\n",
    "\n",
    "def replace_email_address(input_str):\n",
    "    return re.sub(r\"[^\\s]+@[^\\s]+\", 'emailaddr', input_str)\n",
    "\n",
    "def replace_number(input_str):\n",
    "    return re.sub(r\"\\d\", ' number ', input_str)\n",
    "\n",
    "def replace_doller(input_str):\n",
    "    return re.sub(r\"\\$\", 'doller', input_str)\n",
    "\n",
    "def replace_punctuation(input_str):\n",
    "    return re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——!-=，。？、~@;:#￥%……&*\\(\\)\\[\\]]+\", ' ', input_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess_email_content(content, ignore_header=True):\n",
    "    email_content = \"\"\n",
    "    if ignore_header:\n",
    "        msg = email.message_from_string(content)\n",
    "        if msg.is_multipart():\n",
    "            for payload in meg.get_payload():\n",
    "                email_content = email_content + payload.get_payload()\n",
    "        else:\n",
    "            email_content = msg.get_payload()\n",
    "    else:\n",
    "        email_content = content\n",
    "    \n",
    "    email_content = email_content.lower()                # lower case\n",
    "    email_content = strip_http_tags(email_content)       # strip http tags\n",
    "    email_content = replace_url(email_content)           # replace url\n",
    "    email_content = replace_email_address(email_content) # replace email address\n",
    "    email_content = replace_number(email_content)        # replace number\n",
    "    email_content = replace_doller(email_content)        # replace doller\n",
    "    email_content = replace_punctuation(email_content)   # \n",
    "    return email_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create hashmap for list\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "def convert_email_content_to_dict(content, word_dict, enable_word_stemming=True):\n",
    "    for word in content.split():\n",
    "        if enable_word_stemming:\n",
    "            word = ps.stem(word)\n",
    "            \n",
    "        if word in word_dict:\n",
    "            count = word_dict[word]\n",
    "            word_dict[word] = count+1\n",
    "        else:\n",
    "            word_dict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load object\n",
    "import pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'from': 5, 'emailaddr': 5, 'mon': 5, 'sep': 5, 'number': 166, 'return': 1, 'path': 1, 'deliv': 1, 'to': 2, 'receiv': 3, 'localhost': 3, 'jalapeno': 2, 'by': 4, 'jmason': 1, 'org': 3, 'postfix': 1, 'with': 3, 'esmtp': 2, 'id': 3, 'ae': 1, 'f': 1, 'for': 3, 'ist': 2, 'imap': 1, 'fetchmail': 1, 'singl': 1, 'drop': 1, 'dogma': 2, 'slashnul': 2, 'g': 1, 'u': 1, 'fg': 1, 'messag': 1, 'gamasutra': 1, 'subject': 1, 'priceless': 1, 'ruben': 1, 'work': 1, 'stolen': 1, 'in': 2, 'raid': 2, 'on': 2, 'mansion': 1, 'date': 3, 'content': 1, 'type': 1, 'text': 1, 'plain': 1, 'encod': 1, 'utf': 1, 'line': 1, 'x': 2, 'spam': 2, 'statu': 1, 'no': 1, 'hit': 1, 'requir': 1, 'test': 1, 'awl': 1, 'past': 1, 't': 2, 'uri': 1, 'count': 1, 'version': 1, 'cv': 1, 'level': 1, 'url': 1, 'httpaddr': 1, 'click': 1, 'art': 2, 'fourth': 1, 'philanthropist': 1, 's': 1, 'home': 1, 'onc': 1, 'target': 1, 'the': 1, 'ira': 1, 'and': 1, 'dublin': 1, 'gangster': 1, 'martin': 1, 'cahil': 1}\n"
     ]
    }
   ],
   "source": [
    "# From text file to word dict\n",
    "test_file = open(test_file_path, \"r\")\n",
    "file_text = test_file.read()\n",
    "processed_content = preprocess_email_content(file_text, ignore_header=False)\n",
    "# print(processed_content)\n",
    "word_dict = {}\n",
    "convert_email_content_to_dict(processed_content, word_dict)\n",
    "print(word_dict)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22614a9638848c595305f5732bdaff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, description='Processing:', max=6852.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 151385\n",
      "Top ten words:\n",
      "('number', 2531063)\n",
      "('the', 85736)\n",
      "('to', 85228)\n",
      "('emailaddr', 65339)\n",
      "('from', 62193)\n",
      "('a', 60257)\n",
      "('for', 51849)\n",
      "('with', 46690)\n",
      "('by', 45732)\n",
      "('and', 44800)\n",
      "Save dict to ./word_dict.pkl\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "\n",
    "# progress bar\n",
    "progress_bar = FloatProgress(min=0, max=6852, description='Processing:') # instantiate the bar\n",
    "display(progress_bar) # display the bar\n",
    "\n",
    "sample_directories = [\"./easy_ham/\", \"./hard_ham/\", \"./spam/\"]\n",
    "word_dict = {}\n",
    "for directory in sample_directories:\n",
    "    files = os.listdir(directory)\n",
    "    for file in files:\n",
    "        file_path = directory + file\n",
    "#         print(\"file_path:\", file_path)\n",
    "        # process email\n",
    "        email_file = open(file_path, \"r\", errors='ignore')\n",
    "        file_text = email_file.read()\n",
    "        processed_content = preprocess_email_content(file_text, ignore_header=False)\n",
    "        convert_email_content_to_dict(processed_content, word_dict)\n",
    "        email_file.close()\n",
    "        progress_bar.value += 1\n",
    "print(\"Total words:\", len(word_dict))\n",
    "\n",
    "# Sort dict by word count\n",
    "sorted_result = sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "sorted_word_dict = dict(sorted_result)\n",
    "\n",
    "# Print top ten words\n",
    "print(\"Top ten words:\")\n",
    "item_list = list(sorted_word_dict.items())\n",
    "for i in np.arange(0, 10):\n",
    "    print(item_list[i])\n",
    "\n",
    "save_obj(word_dict, \"word_dict\")\n",
    "print(\"Save dict to ./word_dict.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert email content to word indices table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepared vocabulary word list\n",
    "vocabulary_dict = dict(sorted_result[:2500])\n",
    "index = 0\n",
    "for key in vocabulary_dict.keys():\n",
    "    vocabulary_dict[key] = index;\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_email_content_to_word_indices(processed_content, enable_word_stemming=True):\n",
    "    word_indices = []\n",
    "    for word in content.split():\n",
    "        if enable_word_stemming:\n",
    "            word = ps.stem(word)\n",
    "            \n",
    "        if word in vocabulary_dict:\n",
    "            word_indices.append(vocabulary_dict[word])\n",
    "    return word_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 53, 40, 4, 31, 8, 7, 25, 13, 6, 40, 4, 115, 8, 31, 7, 108, 6, 40, 4, 8, 7, 25, 13, 6, 40, 148, 15, 2148, 36, 40, 1794, 2148, 36, 247, 540, 915, 8, 1, 9, 984]\n"
     ]
    }
   ],
   "source": [
    "# From text file to word indices\n",
    "test_file = open(test_file_path, \"r\")\n",
    "file_text = test_file.read()\n",
    "processed_content = preprocess_email_content(file_text, ignore_header=False)\n",
    "test_indices = convert_email_content_to_word_indices(processed_content)\n",
    "print(test_indices)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e70c2b25854e6fa49faadbe3ffd1a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, description='Processing:', max=6852.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# progress bar\n",
    "progress_bar = FloatProgress(min=0, max=6852, description='Processing:') # instantiate the bar\n",
    "display(progress_bar) # display the bar\n",
    "\n",
    "sample_directories = [\"./easy_ham/\", \"./hard_ham/\", \"./spam/\"]\n",
    "summary_file_names = [\"word_indices_easy_ham\", \"word_indices_hard_ham\", \"word_indices_spam\"]\n",
    "for directory, file_name in zip(sample_directories, summary_file_names):\n",
    "    files = os.listdir(directory)\n",
    "    word_indices_table = []\n",
    "    for file in files:\n",
    "        file_path = directory + file\n",
    "#         print(\"file_path:\", file_path)\n",
    "        # process email\n",
    "        email_file = open(file_path, \"r\", errors='ignore')\n",
    "        file_text = email_file.read()\n",
    "        processed_content = preprocess_email_content(file_text, ignore_header=False)\n",
    "        word_indices = convert_email_content_to_word_indices(processed_content)\n",
    "        word_indices_table.append(word_indices)\n",
    "        progress_bar.value += 1\n",
    "    save_obj(word_indices_table, file_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
