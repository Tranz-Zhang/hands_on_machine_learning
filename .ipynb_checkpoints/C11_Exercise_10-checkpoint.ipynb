{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining on an auxiliary task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data. Start by building two DNNs (let’s call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add a single output layer on top of both DNNs. You should use TensorFlow’s concat() function with axis=1 to concatenate the outputs of both DNNs along the horizontal axis, then feed the result to the output layer. This output layer should contain a single neuron using the logistic activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from tensorflow.contrib.layers import dropout\n",
    "\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name=\"is_training\")\n",
    "bn_params = {\n",
    "    'is_training':is_training,\n",
    "    'decay': 0.99,\n",
    "    'updates_collections': None,\n",
    "}\n",
    "keep_prob = 0.9\n",
    "\n",
    "def create_dnn(inputs, n_hidden_layers, n_hiddens, scope_name, enable_dropout=False):\n",
    "    with tf.name_scope(scope_name):\n",
    "        he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "        for layer_idx in range(n_hidden_layers):\n",
    "            dropout_inputs = dropout(inputs, keep_prob, is_training=is_training)\n",
    "            inputs = fully_connected(dropout_inputs,\n",
    "                                     n_hiddens,\n",
    "                                     weights_initializer=he_init, \n",
    "                                     normalizer_fn=batch_norm, \n",
    "                                     normalizer_params=bn_params, \n",
    "                                     activation_fn=tf.nn.elu,\n",
    "                                     scope= scope_name + '_hidden%d' % (layer_idx + 1))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DNN-A\n",
    "X_a = tf.placeholder(tf.float32, shape=(None, 28 * 28), name='X_a')\n",
    "dnn_a = create_dnn(X_a, 5, 100, 'dnn_a')\n",
    "\n",
    "# Create DNN-B\n",
    "X_b = tf.placeholder(tf.float32, shape=(None, 28 * 28), name='X_b')\n",
    "dnn_b = create_dnn(X_b, 5, 100, 'dnn_b')\n",
    "\n",
    "# label\n",
    "y = tf.placeholder(tf.int32, shape=(None), name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 2 Dnns\n",
    "combine_layers = tf.concat([dnn_a, dnn_b], axis=1, name='concat')\n",
    "logits = fully_connected(combine_layers, 2, activation_fn=tf.nn.sigmoid, scope='outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "# optimizer\n",
    "with tf.name_scope('train_op'):\n",
    "    optimizer = tf.train.AdamOptimizer(name='Adam')\n",
    "    train_op = optimizer.minimize(loss)\n",
    "# eval\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Split the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_all = mnist.train.images\n",
    "y_train_all = mnist.train.labels\n",
    "X_a_train = []\n",
    "X_b_train = []\n",
    "y_train = []\n",
    "indices_diff = []\n",
    "for label in np.unique(mnist.train.labels):\n",
    "    indices = np.where(mnist.train.labels == label)\n",
    "    identical_indices_count = len(indices[0]) // 4 * 2\n",
    "    # 提取相同的数字\n",
    "    for idx in range(identical_indices_count // 2):\n",
    "        X_a_train.append(X_train_all[indices[0][idx]])\n",
    "        X_b_train.append(X_train_all[indices[0][idx * 2]])\n",
    "        y_train.append(0)\n",
    "    # 剩余index放入indices_diff\n",
    "    indices_left = indices[0][identical_indices_count:]\n",
    "    for idx in indices_left:\n",
    "        indices_diff.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shuffle and create different samples\n",
    "for i in range(len(indices_diff) // 2):\n",
    "    index1 = indices_diff[i]\n",
    "    index2 = indices_diff[len(indices_diff) // 2 + i]\n",
    "    X_a_train.append(X_train_all[index1])\n",
    "    X_b_train.append(X_train_all[index2])\n",
    "    if y_train_all[index1] == y_train_all[index2]:\n",
    "        y_train.append(0)\n",
    "        print('Same!')\n",
    "    else:\n",
    "        y_train.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle traing data\n",
    "random_indices = np.random.permutation(len(y_train))\n",
    "X_a_train = np.array(X_a_train)[random_indices]\n",
    "X_b_train = np.array(X_b_train)[random_indices]\n",
    "y_train = np.array(y_train)[random_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.76\n",
      "Train accuracy: 0.78\n",
      "Train accuracy: 0.76\n",
      "Train accuracy: 0.88\n",
      "Train accuracy: 0.8\n",
      "Train accuracy: 0.82\n",
      "Train accuracy: 0.9\n",
      "Train accuracy: 0.88\n",
      "Train accuracy: 0.86\n",
      "Train accuracy: 0.84\n",
      "Train accuracy: 0.9\n",
      "Train accuracy: 0.88\n",
      "Train accuracy: 0.86\n",
      "Train accuracy: 0.9\n",
      "Train accuracy: 0.88\n",
      "Train accuracy: 0.9\n",
      "Train accuracy: 0.86\n",
      "Train accuracy: 0.9\n",
      "Train accuracy: 0.88\n",
      "Train accuracy: 0.88\n",
      "Train accuracy: 0.94\n",
      "Train accuracy: 0.9\n",
      "Train accuracy: 0.9\n",
      "Train accuracy: 0.96\n",
      "Train accuracy: 0.94\n",
      "Train accuracy: 0.88\n",
      "Train accuracy: 0.92\n",
      "Train accuracy: 0.94\n",
      "Train accuracy: 0.9\n",
      "Train accuracy: 0.9\n",
      "Train accuracy: 0.9\n",
      "Train accuracy: 0.94\n",
      "Train accuracy: 0.94\n",
      "Train accuracy: 0.94\n",
      "Train accuracy: 0.94\n",
      "Train accuracy: 0.94\n",
      "Train accuracy: 0.94\n",
      "Train accuracy: 0.96\n",
      "Train accuracy: 0.94\n",
      "Train accuracy: 0.92\n",
      "Train accuracy: 0.94\n",
      "Train accuracy: 0.98\n",
      "Train accuracy: 0.98\n",
      "Train accuracy: 0.96\n",
      "Train accuracy: 1.0\n",
      "Train accuracy: 0.98\n",
      "Train accuracy: 0.96\n",
      "Train accuracy: 0.98\n",
      "Train accuracy: 0.92\n",
      "Train accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 50\n",
    "batch_size = 50\n",
    "model_path = '/Users/chancezhang/machine_learning/hands_on_machine_learning/tmp/C11/C11_Ex10.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()    \n",
    "    for epoch in range(n_epoch):\n",
    "        for batch_idx in range(y_train.shape[0] // batch_size):\n",
    "            X_a_batch = X_a_train[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "            X_b_batch = X_b_train[batch_idx * batch_size : (batch_idx + 1) * batch_size] \n",
    "            y_batch  = y_train[batch_idx * batch_size : (batch_idx + 1) * batch_size]\n",
    "            sess.run(train_op, feed_dict={X_a:X_a_batch, X_b:X_b_batch, y:y_batch, is_training:True})\n",
    "        # evalue\n",
    "        acc = accuracy.eval(feed_dict={X_a:X_a_batch, X_b:X_b_batch, y:y_batch, is_training:True})\n",
    "        print('Train accuracy:', acc)\n",
    "    saver.save(sess, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on with 10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset Graph\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.saver.Saver at 0x12271b978>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Graph\n",
    "model_path = '/Users/chancezhang/machine_learning/hands_on_machine_learning/tmp/C11/C11_Ex10.ckpt'\n",
    "tf.train.import_meta_graph(model_path + '.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mn = tf.get_default_graph().get_tensor_by_name('X_a:0')\n",
    "y_mn = tf.placeholder(tf.int32, shape=(None), name='y_mn')\n",
    "is_training = tf.get_default_graph().get_tensor_by_name('is_training:0')\n",
    "\n",
    "# get dnn outputs\n",
    "dnn_a_hidden_5_output = tf.get_default_graph().get_tensor_by_name('dnn_a/dnn_a_hidden5/Elu:0')\n",
    "# create soft max output layer\n",
    "logits_mn = fully_connected(dnn_a_hidden_5_output, 10, scope='output_mn', activation_fn=tf.nn.softmax)\n",
    "\n",
    "# loss\n",
    "with tf.name_scope('loss_mn'):\n",
    "    xentropy_mn = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_mn, logits=logits_mn)\n",
    "    loss_mn = tf.reduce_mean(xentropy_mn, name='loss_mn')\n",
    "\n",
    "# optimizer\n",
    "train_var_list = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='dnn_a_hidden[2345]|output_mn')\n",
    "with tf.name_scope('train_op_mn'):\n",
    "    optimizer_mn = tf.train.AdamOptimizer(name='Adam_mn')\n",
    "    train_op_mn = optimizer_mn.minimize(loss_mn)\n",
    "\n",
    "# eval\n",
    "with tf.name_scope('eval_mn'):\n",
    "    correct = tf.nn.in_top_k(logits_mn, y_mn, 1)\n",
    "    accuracy_mn = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy_mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Prepare data set\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples_per_class = 500\n",
    "X_train_small = []\n",
    "y_train_small = []\n",
    "for value in np.unique(mnist.train.labels):\n",
    "    indices = np.where(mnist.train.labels == value)\n",
    "    np.random.shuffle(indices[0])\n",
    "    for image in mnist.train.images[indices[0][:n_samples_per_class]]:\n",
    "        X_train_small.append(image)\n",
    "        y_train_small.append(value)\n",
    "X_train_small = np.array(X_train_small)\n",
    "y_train_small = np.array(y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrain accuracy: 0.139 test acuracy: 0.1518\n",
      "Retrain accuracy: 0.5428 test acuracy: 0.5417\n",
      "Retrain accuracy: 0.6612 test acuracy: 0.6597\n",
      "Retrain accuracy: 0.8602 test acuracy: 0.8605\n",
      "Retrain accuracy: 0.9178 test acuracy: 0.9084\n",
      "Retrain accuracy: 0.9352 test acuracy: 0.923\n",
      "Retrain accuracy: 0.9456 test acuracy: 0.9327\n",
      "Retrain accuracy: 0.9488 test acuracy: 0.9367\n",
      "Retrain accuracy: 0.9564 test acuracy: 0.9403\n",
      "Retrain accuracy: 0.9616 test acuracy: 0.944\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 500\n",
    "batch_size = 50\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"dnn_a\")\n",
    "restore_saver = tf.train.Saver(reuse_vars)\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, model_path)\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "#         for batch_idx in range(X_train_small.shape[0] // batch_size):\n",
    "#             X_batch = X_train_small[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n",
    "#             y_batch = y_train_small[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n",
    "#             sess.run(train_op_mn, feed_dict={X_mn:X_batch, y_mn:y_batch, is_training:True})\n",
    "        sess.run(train_op_mn, feed_dict={X_mn:X_train_small, y_mn:y_train_small, is_training:True})\n",
    "        acc_train = accuracy_mn.eval(feed_dict={X_mn:X_train_small, y_mn:y_train_small, is_training:False})\n",
    "        acc_test = accuracy_mn.eval(feed_dict={X_mn:mnist.test.images,\n",
    "                                               y_mn:mnist.test.labels,\n",
    "                                               is_training:False})\n",
    "        if epoch % 50 == 0:\n",
    "            print('Retrain accuracy:', acc_train, 'test acuracy:', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 总结：\n",
    "- 小数据正常训练下，最高可达到87%的准确率\n",
    "- 在现有网络基础上，添加输出层，并且全参数训练，可达到94%的准确率\n",
    "- 冻结底层网络模型后，准确度有所降低，甚至达不到普通训练的准确度..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
