{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
    "\n",
    "b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "\n",
    "c. Tune the hyperparameters using cross-validation and see what precision you can achieve.\n",
    "\n",
    "d. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?\n",
    "\n",
    "e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Inputs and outputs\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_outputs = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "bn_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.99,\n",
    "    'updates_collections': None\n",
    "}\n",
    "keep_prob = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-431c184c893b>, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-431c184c893b>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    if\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# build layers\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from tensorflow.contrib.layers import dropout\n",
    "\n",
    "enable_dropout = False\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "with tf.contrib.framework.arg_scope(\n",
    "        [fully_connected],\n",
    "        weights_initializer = he_init,\n",
    "        normalizer_fn =  batch_norm,\n",
    "        normalizer_params = bn_params):\n",
    "    if enable_dropout == True: \n",
    "        hidden_1 = fully_connected(X, n_hidden, scope='hidden1', activation_fn=tf.nn.relu)\n",
    "        hidden_drop_1 = dropout(hidden_1, keep_prob, is_training=is_training)\n",
    "        hidden_2 = fully_connected(hidden_drop_1, n_hidden, scope='hidden2', activation_fn=tf.nn.relu)\n",
    "        hidden_drop_2 = dropout(hidden_2, keep_prob, is_training=is_training)\n",
    "        hidden_3 = fully_connected(hidden_drop_2, n_hidden, scope='hidden3', activation_fn=tf.nn.relu)\n",
    "        hidden_drop_3 = dropout(hidden_3, keep_prob, is_training=is_training)\n",
    "        hidden_4 = fully_connected(hidden_drop_3, n_hidden, scope='hidden4', activation_fn=tf.nn.relu)\n",
    "        hidden_drop_4 = dropout(hidden_4, keep_prob, is_training=is_training)\n",
    "        logits = fully_connected(hidden_drop_4, n_outputs, scope='hidden5', activation_fn=tf.nn.softmax)\n",
    "    else:\n",
    "        hidden_1 = fully_connected(X, n_hidden, scope='hidden1', activation_fn=tf.nn.relu)\n",
    "        hidden_2 = fully_connected(hidden_1, n_hidden, scope='hidden2', activation_fn=tf.nn.relu)\n",
    "        hidden_3 = fully_connected(hidden_2, n_hidden, scope='hidden3', activation_fn=tf.nn.relu)\n",
    "        hidden_4 = fully_connected(hidden_3, n_hidden, scope='hidden4', activation_fn=tf.nn.relu)\n",
    "        logits = fully_connected(hidden_4, n_outputs, scope='hidden5', activation_fn=tf.nn.softmax)\n",
    "    if        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss, name='train_op')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalue, 这一步不影响训练流程，只是为了获取一些信息\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epoch = 50\n",
    "batch_size = 50\n",
    "minimum_loss = 1.11\n",
    "# minimum_loss = 0.913\n",
    "# minimum_loss = 1.49\n",
    "\n",
    "def run_train(sess, X_train, y_train, train_idx):\n",
    "    sess.run(init)\n",
    "#     sess.run(he_init)\n",
    "    model_path = '/Users/chancezhang/machine_learning/hands_on_machine_learning/tmp/C11/C11_Ex08_' + str(train_idx) + '.ckpt'\n",
    "    for epoch in range(n_epoch):\n",
    "        for idx in range(X_train.shape[0] // batch_size):\n",
    "            X_batch = X_train[batch_size * idx : batch_size * (idx + 1)]\n",
    "            y_batch = y_train[batch_size * idx : batch_size * (idx + 1)]\n",
    "            run_ret = sess.run(train_op, feed_dict={X:X_batch, y:y_batch, is_training:True})\n",
    "        runtime_loss = loss.eval(feed_dict={X:X_train, y:y_train, is_training:False})\n",
    "        runtime_acc = accuracy.eval(feed_dict={X:X_train, y:y_train, is_training:False})\n",
    "        print('epoch ', epoch, 'runtime-accuracy:', runtime_acc, 'loss:', runtime_loss)\n",
    "        if runtime_loss < minimum_loss:\n",
    "            save_path = saver.save(sess, model_path) # early-stopping\n",
    "            return;\n",
    "    save_path = saver.save(sess, model_path) # early-stopping\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "    \n",
    "#     for epoch in range(n_epoch):\n",
    "#         for mini_batch_idx in range(mnist.train.num_examples // batch_size):\n",
    "#             X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "#             sess.run(train_op, feed_dict={X:X_batch, y:y_batch})\n",
    "#         acc_train = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "#         acc_test = accuracy.eval(feed_dict={X:mnist.test.images,\n",
    "#                                             y:mnist.test.labels})\n",
    "#         print('epoch ', epoch, 'train_acc:', acc_train, 'test_acc:', acc_test)\n",
    "#     save_path = saver.save(sess, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Implementation\n",
    "split_size = 5\n",
    "from sklearn.model_selection import KFold\n",
    "results = []\n",
    "with tf.Session() as sess:\n",
    "    # 只采用0~4数据\n",
    "    X_train_all = mnist.train.images\n",
    "    y_train_all = mnist.train.labels\n",
    "    selective_indices = np.where(y_train_all < 5)\n",
    "    X_train = X_train_all[selective_indices]\n",
    "    y_train = y_train_all[selective_indices]\n",
    "    \n",
    "    train_id = 0\n",
    "    kf = KFold(n_splits=split_size)\n",
    "    for train_idx, val_idx in kf.split(X_train, y_train):\n",
    "        X_train_cs = X_train[train_idx]\n",
    "        y_train_cs = y_train[train_idx]\n",
    "        X_val_cs = X_train[val_idx]\n",
    "        y_val_cs = y_train[val_idx]\n",
    "        print('Begin Cross-Validation Train: ', train_id)\n",
    "        run_train(sess, X_train_cs, y_train_cs, train_id)\n",
    "        acc = accuracy.eval(feed_dict={X:X_val_cs, y:y_val_cs, is_training:False})\n",
    "        print('accuracy: ', acc)\n",
    "        results.append(acc)\n",
    "        train_id += 1\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save final model\n",
    "with tf.Session() as sess:\n",
    "    print('Begin Final Train')\n",
    "    X_train_all = mnist.train.images\n",
    "    y_train_all = mnist.train.labels\n",
    "    selective_indices = np.where(y_train_all < 5)\n",
    "    X_train = X_train_all[selective_indices]\n",
    "    y_train = y_train_all[selective_indices]\n",
    "    \n",
    "    X_test_all = mnist.test.images\n",
    "    y_test_all = mnist.test.labels\n",
    "    selective_indices = np.where(y_test_all < 5)\n",
    "    X_test = X_test_all[selective_indices]\n",
    "    y_test = y_test_all[selective_indices]\n",
    "    \n",
    "    run_train(sess, X_train, y_train, 100)\n",
    "    acc = accuracy.eval(feed_dict={X:X_test, y:y_test, is_training:False})\n",
    "    print('Final Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
