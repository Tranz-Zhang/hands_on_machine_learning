{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
    "\n",
    "b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
    "\n",
    "c. Tune the hyperparameters using cross-validation and see what precision you can achieve.\n",
    "\n",
    "d. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?\n",
    "\n",
    "e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Inputs and outputs\n",
    "n_inputs = 28 * 28\n",
    "n_hidden = 100\n",
    "n_outputs = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "bn_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.99,\n",
    "    'updates_collections': None\n",
    "}\n",
    "keep_prob = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build layers\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from tensorflow.contrib.layers import dropout\n",
    "\n",
    "enable_dropout = False\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "with tf.contrib.framework.arg_scope(\n",
    "        [fully_connected],\n",
    "        weights_initializer = he_init,\n",
    "        normalizer_fn =  batch_norm,\n",
    "        normalizer_params = bn_params):\n",
    "    if enable_dropout: \n",
    "        hidden_1 = fully_connected(X, n_hidden, scope='hidden1', activation_fn=tf.nn.relu)\n",
    "        hidden_drop_1 = dropout(hidden_1, keep_prob, is_training=is_training)\n",
    "        hidden_2 = fully_connected(hidden_drop_1, n_hidden, scope='hidden2', activation_fn=tf.nn.relu)\n",
    "        hidden_drop_2 = dropout(hidden_2, keep_prob, is_training=is_training)\n",
    "        hidden_3 = fully_connected(hidden_drop_2, n_hidden, scope='hidden3', activation_fn=tf.nn.relu)\n",
    "        hidden_drop_3 = dropout(hidden_3, keep_prob, is_training=is_training)\n",
    "        hidden_4 = fully_connected(hidden_drop_3, n_hidden, scope='hidden4', activation_fn=tf.nn.relu)\n",
    "        hidden_drop_4 = dropout(hidden_4, keep_prob, is_training=is_training)\n",
    "        logits = fully_connected(hidden_drop_4, n_outputs, scope='hidden5', activation_fn=tf.nn.softmax)\n",
    "    else:\n",
    "        hidden_1 = fully_connected(X, n_hidden, scope='hidden1', activation_fn=tf.nn.relu)\n",
    "        hidden_2 = fully_connected(hidden_1, n_hidden, scope='hidden2', activation_fn=tf.nn.relu)\n",
    "        hidden_3 = fully_connected(hidden_2, n_hidden, scope='hidden3', activation_fn=tf.nn.relu)\n",
    "        hidden_4 = fully_connected(hidden_3, n_hidden, scope='hidden4', activation_fn=tf.nn.relu)\n",
    "        logits = fully_connected(hidden_4, n_outputs, scope='hidden5', activation_fn=tf.nn.softmax)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train_op = optimizer.minimize(loss, name='train_op')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalue, 这一步不影响训练流程，只是为了获取一些信息\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-a37a59f11b0d>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epoch = 50\n",
    "batch_size = 50\n",
    "minimum_loss = 1.11\n",
    "# minimum_loss = 0.913\n",
    "# minimum_loss = 1.49\n",
    "\n",
    "def run_train(sess, X_train, y_train, train_idx):\n",
    "    sess.run(init)\n",
    "#     sess.run(he_init)\n",
    "    model_path = '/Users/chancezhang/machine_learning/hands_on_machine_learning/tmp/C11/C11_Ex08_' + str(train_idx) + '.ckpt'\n",
    "    for epoch in range(n_epoch):\n",
    "        for idx in range(X_train.shape[0] // batch_size):\n",
    "            X_batch = X_train[batch_size * idx : batch_size * (idx + 1)]\n",
    "            y_batch = y_train[batch_size * idx : batch_size * (idx + 1)]\n",
    "            run_ret = sess.run(train_op, feed_dict={X:X_batch, y:y_batch, is_training:True})\n",
    "        runtime_loss = loss.eval(feed_dict={X:X_train, y:y_train, is_training:False})\n",
    "        runtime_acc = accuracy.eval(feed_dict={X:X_train, y:y_train, is_training:False})\n",
    "        print('epoch ', epoch, 'runtime-accuracy:', runtime_acc, 'loss:', runtime_loss)\n",
    "        if runtime_loss < minimum_loss:\n",
    "            save_path = saver.save(sess, model_path) # early-stopping\n",
    "            return;\n",
    "    save_path = saver.save(sess, model_path) # early-stopping\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "    \n",
    "#     for epoch in range(n_epoch):\n",
    "#         for mini_batch_idx in range(mnist.train.num_examples // batch_size):\n",
    "#             X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "#             sess.run(train_op, feed_dict={X:X_batch, y:y_batch})\n",
    "#         acc_train = accuracy.eval(feed_dict={X:X_batch, y:y_batch})\n",
    "#         acc_test = accuracy.eval(feed_dict={X:mnist.test.images,\n",
    "#                                             y:mnist.test.labels})\n",
    "#         print('epoch ', epoch, 'train_acc:', acc_train, 'test_acc:', acc_test)\n",
    "#     save_path = saver.save(sess, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Cross-Validation Train:  0\n",
      "epoch  0 runtime-accuracy: 0.9844405 loss: 1.1319643\n",
      "epoch  1 runtime-accuracy: 0.9910834 loss: 1.1255578\n",
      "epoch  2 runtime-accuracy: 0.994115 loss: 1.120471\n",
      "epoch  3 runtime-accuracy: 0.9954525 loss: 1.1166725\n",
      "epoch  4 runtime-accuracy: 0.9967008 loss: 1.1102158\n",
      "epoch  5 runtime-accuracy: 0.99772626 loss: 1.108918\n",
      "accuracy:  0.98555636\n",
      "Begin Cross-Validation Train:  1\n",
      "epoch  0 runtime-accuracy: 0.98082924 loss: 1.1281295\n",
      "epoch  1 runtime-accuracy: 0.9892555 loss: 1.1218494\n",
      "epoch  2 runtime-accuracy: 0.9930896 loss: 1.121137\n",
      "epoch  3 runtime-accuracy: 0.99465 loss: 1.1174667\n",
      "epoch  4 runtime-accuracy: 0.99621046 loss: 1.1157897\n",
      "epoch  5 runtime-accuracy: 0.9943825 loss: 1.1176897\n",
      "epoch  6 runtime-accuracy: 0.99621046 loss: 1.1093544\n",
      "accuracy:  0.989301\n",
      "Begin Cross-Validation Train:  2\n",
      "epoch  0 runtime-accuracy: 0.98319215 loss: 1.1223212\n",
      "epoch  1 runtime-accuracy: 0.99028087 loss: 1.1171403\n",
      "epoch  2 runtime-accuracy: 0.9934017 loss: 1.1157624\n",
      "epoch  3 runtime-accuracy: 0.99594295 loss: 1.112675\n",
      "epoch  4 runtime-accuracy: 0.99687916 loss: 1.1103039\n",
      "epoch  5 runtime-accuracy: 0.9974142 loss: 1.1133564\n",
      "epoch  6 runtime-accuracy: 0.99772626 loss: 1.1102833\n",
      "epoch  7 runtime-accuracy: 0.9974142 loss: 1.1175067\n",
      "epoch  8 runtime-accuracy: 0.99781543 loss: 1.112432\n",
      "epoch  9 runtime-accuracy: 0.9976371 loss: 1.1124393\n",
      "epoch  10 runtime-accuracy: 0.9976817 loss: 1.1115028\n",
      "epoch  11 runtime-accuracy: 0.9981721 loss: 1.1125249\n",
      "epoch  12 runtime-accuracy: 0.9984842 loss: 1.1125442\n",
      "epoch  13 runtime-accuracy: 0.9967454 loss: 1.1138022\n",
      "epoch  14 runtime-accuracy: 0.99777085 loss: 1.1069608\n",
      "accuracy:  0.9876962\n",
      "Begin Cross-Validation Train:  3\n",
      "epoch  0 runtime-accuracy: 0.98381704 loss: 1.1281428\n",
      "epoch  1 runtime-accuracy: 0.9902367 loss: 1.1228547\n",
      "epoch  2 runtime-accuracy: 0.99326825 loss: 1.1182518\n",
      "epoch  3 runtime-accuracy: 0.9951407 loss: 1.1158359\n",
      "epoch  4 runtime-accuracy: 0.99652267 loss: 1.1101304\n",
      "epoch  5 runtime-accuracy: 0.9959431 loss: 1.1107246\n",
      "epoch  6 runtime-accuracy: 0.9967902 loss: 1.1150475\n",
      "epoch  7 runtime-accuracy: 0.9959877 loss: 1.1108212\n",
      "epoch  8 runtime-accuracy: 0.9975926 loss: 1.1052603\n",
      "accuracy:  0.9875156\n",
      "Begin Cross-Validation Train:  4\n",
      "epoch  0 runtime-accuracy: 0.9820338 loss: 1.1313411\n",
      "epoch  1 runtime-accuracy: 0.99041504 loss: 1.124203\n",
      "epoch  2 runtime-accuracy: 0.9924658 loss: 1.1197382\n",
      "epoch  3 runtime-accuracy: 0.99398154 loss: 1.115705\n",
      "epoch  4 runtime-accuracy: 0.9938924 loss: 1.1160165\n",
      "epoch  5 runtime-accuracy: 0.9954527 loss: 1.1122026\n",
      "epoch  6 runtime-accuracy: 0.996701 loss: 1.1146399\n",
      "epoch  7 runtime-accuracy: 0.9964335 loss: 1.1133714\n",
      "epoch  8 runtime-accuracy: 0.9951407 loss: 1.112566\n",
      "epoch  9 runtime-accuracy: 0.9963443 loss: 1.1229314\n",
      "epoch  10 runtime-accuracy: 0.9975926 loss: 1.1126713\n",
      "epoch  11 runtime-accuracy: 0.9980384 loss: 1.1083268\n",
      "accuracy:  0.9907259\n",
      "[0.98555636, 0.989301, 0.9876962, 0.9875156, 0.9907259]\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation Implementation\n",
    "split_size = 5\n",
    "from sklearn.model_selection import KFold\n",
    "results = []\n",
    "with tf.Session() as sess:\n",
    "    # 只采用0~4数据\n",
    "    X_train_all = mnist.train.images\n",
    "    y_train_all = mnist.train.labels\n",
    "    selective_indices = np.where(y_train_all < 5)\n",
    "    X_train = X_train_all[selective_indices]\n",
    "    y_train = y_train_all[selective_indices]\n",
    "    \n",
    "    train_id = 0\n",
    "    kf = KFold(n_splits=split_size)\n",
    "    for train_idx, val_idx in kf.split(X_train, y_train):\n",
    "        X_train_cs = X_train[train_idx]\n",
    "        y_train_cs = y_train[train_idx]\n",
    "        X_val_cs = X_train[val_idx]\n",
    "        y_val_cs = y_train[val_idx]\n",
    "        print('Begin Cross-Validation Train: ', train_id)\n",
    "        run_train(sess, X_train_cs, y_train_cs, train_id)\n",
    "        acc = accuracy.eval(feed_dict={X:X_val_cs, y:y_val_cs, is_training:False})\n",
    "        print('accuracy: ', acc)\n",
    "        results.append(acc)\n",
    "        train_id += 1\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin Final Train\n",
      "epoch  0 runtime-accuracy: 0.9844853 loss: 1.128446\n",
      "epoch  1 runtime-accuracy: 0.9910122 loss: 1.125555\n",
      "epoch  2 runtime-accuracy: 0.9937585 loss: 1.1201246\n",
      "epoch  3 runtime-accuracy: 0.99493545 loss: 1.1182389\n",
      "epoch  4 runtime-accuracy: 0.9961481 loss: 1.1138163\n",
      "epoch  5 runtime-accuracy: 0.99604106 loss: 1.1080765\n",
      "Final Accuracy:  0.99085426\n"
     ]
    }
   ],
   "source": [
    "# Train and save final model\n",
    "with tf.Session() as sess:\n",
    "    print('Begin Final Train')\n",
    "    X_train_all = mnist.train.images\n",
    "    y_train_all = mnist.train.labels\n",
    "    selective_indices = np.where(y_train_all < 5)\n",
    "    X_train = X_train_all[selective_indices]\n",
    "    y_train = y_train_all[selective_indices]\n",
    "    \n",
    "    X_test_all = mnist.test.images\n",
    "    y_test_all = mnist.test.labels\n",
    "    selective_indices = np.where(y_test_all < 5)\n",
    "    X_test = X_test_all[selective_indices]\n",
    "    y_test = y_test_all[selective_indices]\n",
    "    \n",
    "    run_train(sess, X_train, y_train, 100)\n",
    "    acc = accuracy.eval(feed_dict={X:X_test, y:y_test, is_training:False})\n",
    "    print('Final Accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
