{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.1\n"
     ]
    }
   ],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating your first graph and running it in a session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create tensor graph\n",
    "x = tf.Variable(3, name='x')\n",
    "y = tf.Variable(4, name='y')\n",
    "f = x*x*y + y + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create running session\n",
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "result = sess.run(f)\n",
    "print(result)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 使用with声明\n",
    "with tf.Session() as sess:\n",
    "    # 这里自动设置了下面代码执行的上下文，即tf.Session()，而且这个上下文会自动释放\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 全局初始化\n",
    "init = tf.global_variables_initializer() # prepare an init node, 这里只是获取一个初始化函数，并不会跑代码\n",
    "with tf.Session() as sess:\n",
    "    init.run() # 初始化所有变量\n",
    "    result = f.eval()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 全局上下文，仅在Python shell中使用\n",
    "sess = tf.InteractiveSession()\n",
    "init.run()\n",
    "result = f.eval()\n",
    "print(result)\n",
    "sess.close() # 这个全局上下文需要手动关闭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1 = tf.Variable(1)\n",
    "x1.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x2 = tf.Variable(2)\n",
    "x2.graph is graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x2.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w = tf.constant(3)\n",
    "x = w + 2\n",
    "y = x + 5\n",
    "z = x * 3\n",
    "with tf.Session() as sess:\n",
    "    # 分开计算时，x会被计算两次\n",
    "    print(y.eval())\n",
    "    print(z.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    y_val, z_val = sess.run([y, z]) # 这种情况下，x只会被计算一次\n",
    "    print(y_val)\n",
    "    print(z_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "m, n = housing.data.shape\n",
    "housing_data_plus_bias = np.c_[np.ones((m, 1)), housing.data] # add bias feature x0\n",
    "housing_target = housing.target.reshape(-1, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 使用Normal Equation进行计算，因此只需要一条公式：theta = (X.transpose * X).inverse * X.transpose * y\n",
    "X = tf.constant(housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing_target, dtype=tf.float32, name='y')\n",
    "XT = tf.transpose(X)\n",
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT, X)), XT), y)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    theta_value = theta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean squared error\n",
    "y_pred = np.matmul(housing_data_plus_bias, theta_value)\n",
    "mse = np.mean(np.square(y_pred - housing_target))\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用TensorFlow计算的好处就是，可以利用GPU进行计算加速（如果支持GPU计算的话）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "scaled_housing_data = std_scaler.fit_transform(housing.data)\n",
    "scaled_housing_data_plus_bias = np.c_[np.ones((m, 1)), scaled_housing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing_target, dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradient = 2 / m * tf.matmul(tf.transpose(X), error)\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradient)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate mean squared error\n",
    "y_pred = np.matmul(scaled_housing_data_plus_bias, best_theta)\n",
    "mse_value = np.mean(np.square(y_pred - housing_target))\n",
    "print(\"MSE:\", mse_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using autodiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "theta2 = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name='theta2')\n",
    "gradient = tf.gradients(mse, [theta2])[0] # 使用tf自带函数进行导数计算\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using an Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(mse)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing_target, dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradient = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# momentum 动量？\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9) # 替换了这里，优化\n",
    "training_op = optimizer.minimize(mse)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, \"MSE =\", mse.eval())\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow for Gradient Descent\n",
    "1. 声明Variable变量：包括X, y, theta\n",
    "2. 构建Cost Function，这里是MSE公式\n",
    "3. 计算gradient，这里使用tf.gradients\n",
    "4. 构建training_op，也就是每次迭代的更新点：theta = theta - gradient * learning_rate\n",
    "5. 获取Variable初始化方法，tf.global_variables_initializer()\n",
    "6. 运行Session，记得先初始化，然后用loop进行多次迭代更新theta值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feeding Data to the Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = tf.placeholder(tf.float32, shape=(None, 3))\n",
    "B = A + 5\n",
    "with tf.Session() as sess:\n",
    "    B_val_1 = B.eval(feed_dict={A: [[1, 2, 3]]})\n",
    "    B_val_2 = B.eval(feed_dict={A: [[4, 5, 6], [7, 8, 9]]})\n",
    "print(B_val_1)\n",
    "print(B_val_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Difference between Variable and Placeholder:\n",
    "https://stackoverflow.com/questions/36693740/whats-the-difference-between-tf-placeholder-and-tf-variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Mini-batch Gradient Descent\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "# X = tf.Variable(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "# y = tf.Variable(housing_target, dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], 1.0 -1.0), name=\"theta\");\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "mse = tf.reduce_mean(tf.square(y_pred - y), name='mse')\n",
    "gradient = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradient)\n",
    "\n",
    "n_epochs=3\n",
    "batch_size=100 # 每次获取的数据块大小\n",
    "n_batchs = int(np.floor(m / batch_size))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "last_epoch = -1\n",
    "random_indices = np.random.permutation(m)\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "#     print('fetch_batch', epoch, batch_index, batch_size)\n",
    "    global last_epoch\n",
    "    global random_indices\n",
    "    if last_epoch != epoch:\n",
    "        random_indices = np.random.permutation(m)\n",
    "#         print('Change random indices')\n",
    "        last_epoch = epoch\n",
    "    indices = random_indices[batch_index * batch_size : (batch_index + 1) * batch_size]\n",
    "    # get data\n",
    "    return scaled_housing_data_plus_bias[indices, :], housing_target[indices, :]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        # get batch data            \n",
    "        for batch_index in range(n_batchs):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "#             print(\"X_batch:\", X_batch.shape, \"y_batch:\", y_batch.shape)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        print('Epoch', epoch, \"MSE =\", mse.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        # print info\n",
    "        best_theta = theta.eval()\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Restoring Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个训练节点都保存模型\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "X = tf.constant(scaled_housing_data_plus_bias, dtype=tf.float32, name='X')\n",
    "y = tf.constant(housing_target, dtype=tf.float32, name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], -1.0, 1.0), name='theta')\n",
    "y_pred = tf.matmul(X, theta, name='predictions')\n",
    "error = y_pred - y\n",
    "mse = tf.reduce_mean(tf.square(error), name='mse')\n",
    "gradient = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradient)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch', epoch, \"MSE =\", mse.eval())\n",
    "            save_path = saver.save(sess, '/Users/chance/machine_learning/hands_on_machine_learning/tmp/my_model.ckpt')\n",
    "        sess.run(training_op)\n",
    "    best_theta = theta.eval()\n",
    "    save_path = saver.save(sess, '/Users/chance/machine_learning/hands_on_machine_learning/tmp/my_final.ckpt')\n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore model\n",
    "# 这里恢复的时候不需要调用global_variable_initializer\n",
    "# 模型节点的前置声明还是需要的，\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '/Users/chance/machine_learning/hands_on_machine_learning/tmp/my_final.ckpt')\n",
    "    print(theta.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Graph and Training Curves Using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch Gradient Descent\n",
    "\n",
    "n_epochs = 3\n",
    "learning_rate = 0.01\n",
    "batch_size=100 # 每次获取的数据块大小\n",
    "n_batchs = int(np.floor(m / batch_size))\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n + 1), name='X')\n",
    "y = tf.placeholder(tf.float32, shape=(None, 1), name='y')\n",
    "theta = tf.Variable(tf.random_uniform([n + 1, 1], 1.0 -1.0), name=\"theta\");\n",
    "y_pred = tf.matmul(X, theta, name=\"predictions\")\n",
    "with tf.name_scope(\"loss\") as scope: # 使用scope之后可以把scope里面的操作进行归类\n",
    "    mse = tf.reduce_mean(tf.square(y_pred - y), name='mse')\n",
    "    print(\"Test scope name:\", mse.op.name)\n",
    "gradient = tf.gradients(mse, [theta])[0]\n",
    "training_op = tf.assign(theta, theta - learning_rate * gradient)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "last_epoch = -1\n",
    "random_indices = np.random.permutation(m)\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    global last_epoch\n",
    "    global random_indices\n",
    "    if last_epoch != epoch:\n",
    "        random_indices = np.random.permutation(m)\n",
    "        last_epoch = epoch\n",
    "    indices = random_indices[batch_index * batch_size : (batch_index + 1) * batch_size]\n",
    "    # get data\n",
    "    return scaled_housing_data_plus_bias[indices, :], housing_target[indices, :]\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_dir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}\".format(root_dir, now) # 这里的地址是相对路径的，跟前面的模型保存还不一样...?\n",
    "mse_summary = tf.summary.scalar(\"MSE\", mse)\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(n_epochs):\n",
    "        # get batch data            \n",
    "        for batch_index in range(n_batchs):\n",
    "            X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)  \n",
    "            # 添加文件日志信息\n",
    "            if batch_index % 10 == 0:\n",
    "                summary_str = mse_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                step = epoch * n_batchs + batch_index\n",
    "#                 print(\"step:\", step)\n",
    "                file_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        print('Epoch', epoch, \"MSE =\", mse.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        # print info\n",
    "        best_theta = theta.eval()\n",
    "file_writer.flush()\n",
    "file_writer.close()        \n",
    "print(best_theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moduarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重复的代码\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "w1 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights1\")\n",
    "w2 = tf.Variable(tf.random_normal((n_features, 1)), name=\"weights2\")\n",
    "b1 = tf.Variable(0.0, name=\"bias1\")\n",
    "b2 = tf.Variable(0.0, name=\"bias2\")\n",
    "\n",
    "z1 = tf.add(tf.matmul(X, w1), b1, name=\"z1\")\n",
    "z2 = tf.add(tf.matmul(X, w2), b2, name=\"z1\")\n",
    "\n",
    "relu1 = tf.maximum(z1, 0.0, name=\"relu1\")\n",
    "relu2 = tf.maximum(z2, 0.0, name=\"relu1\")\n",
    "\n",
    "output = tf.add(relu1, relu2, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模块化 \n",
    "def relu(X):\n",
    "    # 也可使用scope封装，graph看起来更有条理\n",
    "    # with tf.name_scope('relu'):\n",
    "    #    ...\n",
    "    w_shape = (int(X.get_shape()[1]), 1) # = (n_features, 1)\n",
    "    w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "    b = tf.Variable(0.0, name=\"bias\")\n",
    "    z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "    return tf.maximum(z, 0, name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "relus = [relu(X) for i in range(5)] # 这里使用python的语法创建5个相同的节点\n",
    "output = tf.add_n(relus, name=\"output\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法1：通过函数参数传递共享参数\n",
    "\n",
    "def relu(X, threshold):\n",
    "    with tf.name_scope('relu'):\n",
    "        w_shape = (int(X.get_shape()[1]), 1) # = (n_features, 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "relus = [relu(X, threshold) for i in range(5)] # 这里使用python的语法创建5个相同的节点\n",
    "output = tf.add_n(relus, name=\"output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法2：在函数内部创建参数\n",
    "\n",
    "def relu(X, threshold):\n",
    "    with tf.name_scope('relu'):\n",
    "        # 这里通过添加python function参数的方式实现...\n",
    "        if not hasattr(relu, 'threshold'):\n",
    "            relu.threshold = tf.Variable(0.0, name=\"threshold\")\n",
    "        w_shape = (int(X.get_shape()[1]), 1) # = (n_features, 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, relu.threshold, name=\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable scope\n",
    "# 这个方式可以防止threshold参数被重用，就是说在‘relu’这个域名下只能有一个threshold参数，重复调用会引发Exception\n",
    "with tf.variable_scope('relu'):\n",
    "    threshold = tf.get_variable('threshold', shape=(), initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引起Exception\n",
    "with tf.variable_scope('relu'):\n",
    "    threshold = tf.get_variable('threshold', shape=(), initializer=tf.constant_initializer(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启用复用功能\n",
    "# 注意：这里的threshold必须在之前已经被初始化过的，否则会引发Exception，也就是说不会自动初始化\n",
    "with tf.variable_scope('relu') as scope:\n",
    "    scope.reuse_variables()\n",
    "    threshold = tf.get_variable('threshold')\n",
    "    threshold.value = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('relu', reuse=True):\n",
    "    threshold = tf.get_variable('threshold')\n",
    "    print(threshold.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：一旦启用了resue之后，便无法关闭，在这个scope下的get_variable都会自动reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法3： 使用variable_scope\n",
    "def relu(X):\n",
    "    with tf.variable_scope('relu', reuse=True):\n",
    "        threshold = tf.get_variable('threshold_')\n",
    "        w_shape = (int(X.get_shape()[1]), 1) # = (n_features, 1)\n",
    "        w = tf.Variable(tf.random_normal(w_shape), name=\"weights\")\n",
    "        b = tf.Variable(0.0, name=\"bias\")\n",
    "        z = tf.add(tf.matmul(X, w), b, name=\"z\")\n",
    "        return tf.maximum(z, threshold, name=\"relu\")\n",
    "\n",
    "n_features = 3\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_features), name=\"X\")\n",
    "with tf.variable_scope('relu'):\n",
    "    threshold = tf.get_variable('threshold_', shape=(), initializer=tf.constant_initializer(0.0))\n",
    "relus = [relu(X) for i in range(5)] # 这里使用python的语法创建5个相同的节点\n",
    "output = tf.add_n(relus, name=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
